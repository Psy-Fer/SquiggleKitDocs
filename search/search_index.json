{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SquiggleKit Docs SquiggleKit is a toolkit for accessing and manipulating nanopore signal data. It has also been designed to act as an introduction to the data files produced by Oxford Nanopore Technology sequencing devices. Code: SquiggleKit . Publication: in progress Overview The toolkit is broken down into two areas: File management fast5_fetcher.py SquigglePull.py Signal analysis segmenter.py MotifSeq.py Getting help with SquiggleKit Happy to help and receive feedback. You can leave an issue on the github repo: SquiggleKit , @Psy_Fer_ on twitter, or shoot mean email at j.ferguson@garvan.org.au Created and maintained by James Ferguson from Genomic Technologies, Kinghorn Centre for Clinical Genomics, Garvan Institute , Sydney Australia.","title":"Home"},{"location":"#welcome-to-squigglekit-docs","text":"SquiggleKit is a toolkit for accessing and manipulating nanopore signal data. It has also been designed to act as an introduction to the data files produced by Oxford Nanopore Technology sequencing devices. Code: SquiggleKit . Publication: in progress","title":"Welcome to SquiggleKit Docs"},{"location":"#overview","text":"The toolkit is broken down into two areas:","title":"Overview"},{"location":"#file-management","text":"fast5_fetcher.py SquigglePull.py","title":"File management"},{"location":"#signal-analysis","text":"segmenter.py MotifSeq.py","title":"Signal analysis"},{"location":"#getting-help-with-squigglekit","text":"Happy to help and receive feedback. You can leave an issue on the github repo: SquiggleKit , @Psy_Fer_ on twitter, or shoot mean email at j.ferguson@garvan.org.au Created and maintained by James Ferguson from Genomic Technologies, Kinghorn Centre for Clinical Genomics, Garvan Institute , Sydney Australia.","title":"Getting help with SquiggleKit"},{"location":"MotifSeq/","text":"MotifSeq MotifSeq docs","title":"MotifSeq"},{"location":"MotifSeq/#motifseq","text":"MotifSeq docs","title":"MotifSeq"},{"location":"SquigglePull/","text":"SquigglePull Background Up until early 2019, Oxford Nanopore sequencing devices stored the raw current measurements and associated metadata in a single Hierarchical Data Format (HDF5) format file, called a fast5 file. After early 2019 devices were updated to produce multi-fast5 files, which contained multiple reads, usually 4000, in a single file. Processing fast5 files has been troublesome for a number of reasons: Unfamiliar format to many people. Need 3rd party library to open and read (h5py, pytables). libraries are not thread safe and locks files. Don't compress as well when single files. (better in multi-fast5) SquigglePull outputs a single tab separated value (.tsv) file where each row contains a single signal and read selected metadata. Current format is designed as an example of producing a more accessible file format for those wishing to get started with nanopore signal data. Columns and data inclusion is subject to change depending on use case. Getting Started SquigglePull can extract both raw current measurements as well as event data. It also has some basic arguments and code scaffolding extraction profiles. This allows the user to implement their analysis methods into the extraction protocol for integration with pipelines, or just quick data trimming. For example using the form -f pos1 , and the tarting -t 20,110 , only the signal values between 20 and 110 will be extracted. Inputs SquigglePull takes 3 mandatory arguments path - Top directory of fast5 files form - Format of targeting information (default: all) raw/event - Raw signal or event data Instructions for use Simply point SquigglePull to a top directory containing fast5 files, and the signal will be extracted to STDOUT Quick start Extract all raw signal python SquigglePull.py -r -p test/R9_raw_data/ data.tsv Extract events between position 20 and 210 python SquigglePull.py -e -p test/R9_event_data/ -t 20,110 -f pos1 data.tsv Full usage usage: SquigglePull.py [-h] [-p PATH] [-t TARGET] [-f {pos1,all}] [-r | -e] [-v] [-s] SquigglePull - extraction of raw/event signal from Oxford Nanopore fast5 files optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Top directory path of fast5 files -t TARGET, --target TARGET Target information as comma delimited string structured by format type -f {pos1,all}, --form {pos1,all} Format of target information -r, --raw Target raw signal -e, --event Target event signal -v, --verbose Engage higher output verbosity -s, --scale Scale signal output for comparison","title":"SquigglePull"},{"location":"SquigglePull/#squigglepull","text":"","title":"SquigglePull"},{"location":"SquigglePull/#background","text":"Up until early 2019, Oxford Nanopore sequencing devices stored the raw current measurements and associated metadata in a single Hierarchical Data Format (HDF5) format file, called a fast5 file. After early 2019 devices were updated to produce multi-fast5 files, which contained multiple reads, usually 4000, in a single file. Processing fast5 files has been troublesome for a number of reasons: Unfamiliar format to many people. Need 3rd party library to open and read (h5py, pytables). libraries are not thread safe and locks files. Don't compress as well when single files. (better in multi-fast5) SquigglePull outputs a single tab separated value (.tsv) file where each row contains a single signal and read selected metadata. Current format is designed as an example of producing a more accessible file format for those wishing to get started with nanopore signal data. Columns and data inclusion is subject to change depending on use case.","title":"Background"},{"location":"SquigglePull/#getting-started","text":"SquigglePull can extract both raw current measurements as well as event data. It also has some basic arguments and code scaffolding extraction profiles. This allows the user to implement their analysis methods into the extraction protocol for integration with pipelines, or just quick data trimming. For example using the form -f pos1 , and the tarting -t 20,110 , only the signal values between 20 and 110 will be extracted.","title":"Getting Started"},{"location":"SquigglePull/#inputs","text":"SquigglePull takes 3 mandatory arguments path - Top directory of fast5 files form - Format of targeting information (default: all) raw/event - Raw signal or event data","title":"Inputs"},{"location":"SquigglePull/#instructions-for-use","text":"Simply point SquigglePull to a top directory containing fast5 files, and the signal will be extracted to STDOUT","title":"Instructions for use"},{"location":"SquigglePull/#quick-start","text":"Extract all raw signal python SquigglePull.py -r -p test/R9_raw_data/ data.tsv Extract events between position 20 and 210 python SquigglePull.py -e -p test/R9_event_data/ -t 20,110 -f pos1 data.tsv","title":"Quick start"},{"location":"SquigglePull/#full-usage","text":"usage: SquigglePull.py [-h] [-p PATH] [-t TARGET] [-f {pos1,all}] [-r | -e] [-v] [-s] SquigglePull - extraction of raw/event signal from Oxford Nanopore fast5 files optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Top directory path of fast5 files -t TARGET, --target TARGET Target information as comma delimited string structured by format type -f {pos1,all}, --form {pos1,all} Format of target information -r, --raw Target raw signal -e, --event Target event signal -v, --verbose Engage higher output verbosity -s, --scale Scale signal output for comparison","title":"Full usage"},{"location":"examples/","text":"Examples The tools can be used in a variety of ways using different wrapping techniques. I have provided a few examples here. Barcode experiment partitioning with fast5_fetcher Fast5 Fetcher was originally built to work with Sun Grid Engine (SGE), exploiting the heck out of array jobs. Although it can work locally and on untarred file structures, when operating on multiple sequencing experiments, with file structures scattered across a file system, is when fast5 fetcher starts to make a difference. SGE examples After creating the fastq/paf/flat, sequencing_summary, and index files, create an SGE file. Note the use of ${SGE_TASK_ID} to use the array job as the pointer to a particular file After barcode demultiplexing Given a similar structure and naming convention, it is possible to group the fast5 files by barcode in the following manner. \u251c\u2500\u2500 BC_1.fastq.gz # Barcode 1 \u251c\u2500\u2500 BC_2.fastq.gz # Barcode 2 \u251c\u2500\u2500 BC_3.fastq.gz # ... \u251c\u2500\u2500 BC_4.fastq.gz \u251c\u2500\u2500 BC_5.fastq.gz \u251c\u2500\u2500 BC_6.fastq.gz \u251c\u2500\u2500 BC_7.fastq.gz \u251c\u2500\u2500 BC_8.fastq.gz \u251c\u2500\u2500 BC_9.fastq.gz \u251c\u2500\u2500 BC_10.fastq.gz \u251c\u2500\u2500 BC_11.fastq.gz \u251c\u2500\u2500 BC_12.fastq.gz \u251c\u2500\u2500 unclassified.fastq.gz # unclassified reads (skipped by fast5_fetcher in this example, rename BC_13 to simple fold it into the example) \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 barcoded.index.gz # index file containing fast5 file paths \u251c\u2500\u2500 fast5/ # fast5 folder, unsorted | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... fetch.sge # activate virtual python environment # most HPC will use something like module load source ~/work/venv2714/bin/activate # Creaete output directory to take advantage of NVME drives on cluster local mkdir ${TMPDIR}/fast5 # Run fast_fetcher on each barcode after demultiplexing time python fast5_fetcher.py -r ./BC_${SGE_TASK_ID}.fastq.gz -s sequencing_summary.txt.gz -i barcoded.index.gz -o ${TMPDIR}/fast5/ # tarball the extracted reads into a single tar file # Can also split the reads into groups of ~4000 if needed tar -cf ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 # Copy from HPC drives to working dir. cp ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar ./ Create CMD and launch # current working dir, with 1 CPU, array jobs 1 to 12 # Modify memory settings as required CMD= qsub -cwd -V -pe smp 1 -N F5F -S /bin/bash -t 1-12 -l mem_requested=20G,h_vmem=20G,tmp_requested=500G ./fetch.sge echo $CMD $CMD Raw signal extraction for analysis Stall detection in raw data Nanopore Adapter detection in raw data Pipelines","title":"Examples"},{"location":"examples/#examples","text":"The tools can be used in a variety of ways using different wrapping techniques. I have provided a few examples here.","title":"Examples"},{"location":"examples/#barcode-experiment-partitioning-with-fast5_fetcher","text":"Fast5 Fetcher was originally built to work with Sun Grid Engine (SGE), exploiting the heck out of array jobs. Although it can work locally and on untarred file structures, when operating on multiple sequencing experiments, with file structures scattered across a file system, is when fast5 fetcher starts to make a difference.","title":"Barcode experiment partitioning with fast5_fetcher"},{"location":"examples/#sge-examples","text":"After creating the fastq/paf/flat, sequencing_summary, and index files, create an SGE file. Note the use of ${SGE_TASK_ID} to use the array job as the pointer to a particular file","title":"SGE examples"},{"location":"examples/#after-barcode-demultiplexing","text":"Given a similar structure and naming convention, it is possible to group the fast5 files by barcode in the following manner. \u251c\u2500\u2500 BC_1.fastq.gz # Barcode 1 \u251c\u2500\u2500 BC_2.fastq.gz # Barcode 2 \u251c\u2500\u2500 BC_3.fastq.gz # ... \u251c\u2500\u2500 BC_4.fastq.gz \u251c\u2500\u2500 BC_5.fastq.gz \u251c\u2500\u2500 BC_6.fastq.gz \u251c\u2500\u2500 BC_7.fastq.gz \u251c\u2500\u2500 BC_8.fastq.gz \u251c\u2500\u2500 BC_9.fastq.gz \u251c\u2500\u2500 BC_10.fastq.gz \u251c\u2500\u2500 BC_11.fastq.gz \u251c\u2500\u2500 BC_12.fastq.gz \u251c\u2500\u2500 unclassified.fastq.gz # unclassified reads (skipped by fast5_fetcher in this example, rename BC_13 to simple fold it into the example) \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 barcoded.index.gz # index file containing fast5 file paths \u251c\u2500\u2500 fast5/ # fast5 folder, unsorted | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ...","title":"After barcode demultiplexing"},{"location":"examples/#fetchsge","text":"# activate virtual python environment # most HPC will use something like module load source ~/work/venv2714/bin/activate # Creaete output directory to take advantage of NVME drives on cluster local mkdir ${TMPDIR}/fast5 # Run fast_fetcher on each barcode after demultiplexing time python fast5_fetcher.py -r ./BC_${SGE_TASK_ID}.fastq.gz -s sequencing_summary.txt.gz -i barcoded.index.gz -o ${TMPDIR}/fast5/ # tarball the extracted reads into a single tar file # Can also split the reads into groups of ~4000 if needed tar -cf ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 # Copy from HPC drives to working dir. cp ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar ./","title":"fetch.sge"},{"location":"examples/#create-cmd-and-launch","text":"# current working dir, with 1 CPU, array jobs 1 to 12 # Modify memory settings as required CMD= qsub -cwd -V -pe smp 1 -N F5F -S /bin/bash -t 1-12 -l mem_requested=20G,h_vmem=20G,tmp_requested=500G ./fetch.sge echo $CMD $CMD","title":"Create CMD and launch"},{"location":"examples/#raw-signal-extraction-for-analysis","text":"","title":"Raw signal extraction for analysis"},{"location":"examples/#stall-detection-in-raw-data","text":"","title":"Stall detection in raw data"},{"location":"examples/#nanopore-adapter-detection-in-raw-data","text":"","title":"Nanopore Adapter detection in raw data"},{"location":"examples/#pipelines","text":"","title":"Pipelines"},{"location":"fast5_fetcher/","text":"fast5_fetcher Doing the heavy lifting for you. fast5_fetcher is a tool for fetching nanopore fast5 files to save time and simplify downstream analysis. Background Reducing the number of fast5 files per folder in a single experiment was a welcomed addition to MinKnow. However this also made it rather useful for manual basecalling on a cluster, using array jobs, where each folder is basecalled individually, producing its own sequencing_summary.txt , reads.fastq , and reads folder containing the newly basecalled fast5s. Taring those fast5 files up into a single file was needed to keep the sys admins at bay, complaining about our millions of individual files on their drives. This meant, whenever there was a need to use the fast5 files from an experiment, or many experiments, unpacking the fast5 files was a significant hurdle both in time and disk space. fast5_fetcher was built to address this bottleneck. By building an index file of the tarballs, and using the sequencing_summary.txt file to match readIDs with fast5 filenames, only the fast5 files you need can be extracted, either temporarily in a pipeline, or permanently, reducing space and simplifying downstream work flows. Getting Started Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure. File structures See Files and folders section Inputs It takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz) 1. fastq, paf, or flat This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2. This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is just a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. See examples below for example test cases. 2. Sequencing summary The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not. 3. Building the index How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. Tarred is preferred. - Raw structure (not preferred) for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index - Local basecalled structure for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index - Parallel basecalled structure for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index Instructions for use If using MacOS, and NOT using homebrew, install it here: https://brew.sh/ then install gnu-tar with: brew install gnu-tar Quick start Basic use on a local computer fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 See examples below for use on an HPC using SGE fast5_fetcher.py Full usage usage: fast5_fetcher.py [-h] [-q FASTQ | -p PAF | -f FLAT] [--OSystem OSYSTEM] [-s SEQ_SUM] [-i INDEX] [-o OUTPUT] [-t] [-l TRIM_LIST] [-x PREFIX] [-z] fast_fetcher - extraction of specific nanopore fast5 files optional arguments: -h, --help show this help message and exit -q FASTQ, --fastq FASTQ fastq.gz for read ids -p PAF, --paf PAF paf alignment file for read ids -f FLAT, --flat FLAT flat file of read ids --OSystem OSYSTEM running operating system - leave default unless doing odd stuff -s SEQ_SUM, --seq_sum SEQ_SUM sequencing_summary.txt.gz file -i INDEX, --index INDEX index.gz file mapping fast5 files in tar archives -o OUTPUT, --output OUTPUT output directory for extracted fast5s -t, --trim trim files as if standalone experiment, (fq, SS) -l TRIM_LIST, --trim_list TRIM_LIST list of file names to trim, comma separated. fastq only needed for -p and -f modes -x PREFIX, --prefix PREFIX trim file prefix, eg: barcode_01, output: barcode_01.fastq, barcode_01_seq_sum.txt -z, --pppp Print out tar commands in batches for further processing Trimming fastq and sequencing_summary files By using the -t, --trim option, each barcode will also have its own sequencing_summary file for downstream analysis. This is particularly useful if each barcode is a different sample or experiment, as the output is as if it was it's own individual flowcell. This method can also trim fastq, and sequencing_summary files when using the paf or flat methods. By using the prefix option, you can label the output names, otherwise generic defaults will be used. batch_tater.py Potato scripting engaged This is designed to run on the output files from fast5_fetcher.py using option -z . This writes out file lists for each tarball that contains reads you want to process. Then batch_tater.py can read those files, to open the individual tar files, and extract the files, meaning the file is only opened once. A recent test using the -z option on ~2.2Tb of data, across ~11/27 million files took about 10min (1CPU) to write and organise the file lists with fast5_fetch.py, and about 20s per array job to extract and repackage with batch_tater.py. This is best used when you want to do something all at once and filter your reads. Other approaches may be better when you are demultiplexing. Usage: Run on SGE using array jobs as a hacky way of doing multiprocessing. Also, helps check when things go wrong, and easy to relaunch failed jobs. batch.sge source ~/work/venv2714/bin/activate FILE=$(ls ./fast5/ | sed -n ${SGE_TASK_ID}p) BLAH=fast5/${FILE} mkdir ${TMPDIR}/fast5 time python batch_tater.py tater_master.txt ${BLAH} ${TMPDIR}/fast5/ echo size of files: 2 du -shc ${TMPDIR}/fast5/ 2 echo extraction complete! 2 echo Number of files: 2 ls ${TMPDIR}/fast5/ | wc -l 2 echo copying data... 2 tar -cf ${TMPDIR}/batch.${SGE_TASK_ID}.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 cp ${TMPDIR}/batch.${SGE_TASK_ID}.tar ./batched_fast5/ Create CMD and launch CMD= qsub -cwd -V -pe smp 1 -N batch -S /bin/bash -t 1-10433 -tc 80 -l mem_requested=20G,h_vmem=20G,tmp_requested=200G ../batch.sge echo $CMD $CMD","title":"fast5_fetcher"},{"location":"fast5_fetcher/#fast5_fetcher","text":"","title":"fast5_fetcher"},{"location":"fast5_fetcher/#doing-the-heavy-lifting-for-you","text":"fast5_fetcher is a tool for fetching nanopore fast5 files to save time and simplify downstream analysis.","title":"Doing the heavy lifting for you."},{"location":"fast5_fetcher/#background","text":"Reducing the number of fast5 files per folder in a single experiment was a welcomed addition to MinKnow. However this also made it rather useful for manual basecalling on a cluster, using array jobs, where each folder is basecalled individually, producing its own sequencing_summary.txt , reads.fastq , and reads folder containing the newly basecalled fast5s. Taring those fast5 files up into a single file was needed to keep the sys admins at bay, complaining about our millions of individual files on their drives. This meant, whenever there was a need to use the fast5 files from an experiment, or many experiments, unpacking the fast5 files was a significant hurdle both in time and disk space. fast5_fetcher was built to address this bottleneck. By building an index file of the tarballs, and using the sequencing_summary.txt file to match readIDs with fast5 filenames, only the fast5 files you need can be extracted, either temporarily in a pipeline, or permanently, reducing space and simplifying downstream work flows.","title":"Background"},{"location":"fast5_fetcher/#getting-started","text":"Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure.","title":"Getting Started"},{"location":"fast5_fetcher/#file-structures","text":"See Files and folders section","title":"File structures"},{"location":"fast5_fetcher/#inputs","text":"It takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz)","title":"Inputs"},{"location":"fast5_fetcher/#1-fastq-paf-or-flat","text":"This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2. This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is just a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. See examples below for example test cases.","title":"1. fastq, paf, or flat"},{"location":"fast5_fetcher/#2-sequencing-summary","text":"The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not.","title":"2. Sequencing summary"},{"location":"fast5_fetcher/#3-building-the-index","text":"How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. Tarred is preferred.","title":"3. Building the index"},{"location":"fast5_fetcher/#-raw-structure-not-preferred","text":"for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index","title":"- Raw structure (not preferred)"},{"location":"fast5_fetcher/#-local-basecalled-structure","text":"for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index","title":"- Local basecalled structure"},{"location":"fast5_fetcher/#-parallel-basecalled-structure","text":"for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index","title":"- Parallel basecalled structure"},{"location":"fast5_fetcher/#instructions-for-use","text":"If using MacOS, and NOT using homebrew, install it here: https://brew.sh/ then install gnu-tar with: brew install gnu-tar","title":"Instructions for use"},{"location":"fast5_fetcher/#quick-start","text":"Basic use on a local computer fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 See examples below for use on an HPC using SGE","title":"Quick start"},{"location":"fast5_fetcher/#fast5_fetcherpy","text":"","title":"fast5_fetcher.py"},{"location":"fast5_fetcher/#full-usage","text":"usage: fast5_fetcher.py [-h] [-q FASTQ | -p PAF | -f FLAT] [--OSystem OSYSTEM] [-s SEQ_SUM] [-i INDEX] [-o OUTPUT] [-t] [-l TRIM_LIST] [-x PREFIX] [-z] fast_fetcher - extraction of specific nanopore fast5 files optional arguments: -h, --help show this help message and exit -q FASTQ, --fastq FASTQ fastq.gz for read ids -p PAF, --paf PAF paf alignment file for read ids -f FLAT, --flat FLAT flat file of read ids --OSystem OSYSTEM running operating system - leave default unless doing odd stuff -s SEQ_SUM, --seq_sum SEQ_SUM sequencing_summary.txt.gz file -i INDEX, --index INDEX index.gz file mapping fast5 files in tar archives -o OUTPUT, --output OUTPUT output directory for extracted fast5s -t, --trim trim files as if standalone experiment, (fq, SS) -l TRIM_LIST, --trim_list TRIM_LIST list of file names to trim, comma separated. fastq only needed for -p and -f modes -x PREFIX, --prefix PREFIX trim file prefix, eg: barcode_01, output: barcode_01.fastq, barcode_01_seq_sum.txt -z, --pppp Print out tar commands in batches for further processing","title":"Full usage"},{"location":"fast5_fetcher/#trimming-fastq-and-sequencing_summary-files","text":"By using the -t, --trim option, each barcode will also have its own sequencing_summary file for downstream analysis. This is particularly useful if each barcode is a different sample or experiment, as the output is as if it was it's own individual flowcell. This method can also trim fastq, and sequencing_summary files when using the paf or flat methods. By using the prefix option, you can label the output names, otherwise generic defaults will be used.","title":"Trimming fastq and sequencing_summary files"},{"location":"fast5_fetcher/#batch_taterpy","text":"Potato scripting engaged This is designed to run on the output files from fast5_fetcher.py using option -z . This writes out file lists for each tarball that contains reads you want to process. Then batch_tater.py can read those files, to open the individual tar files, and extract the files, meaning the file is only opened once. A recent test using the -z option on ~2.2Tb of data, across ~11/27 million files took about 10min (1CPU) to write and organise the file lists with fast5_fetch.py, and about 20s per array job to extract and repackage with batch_tater.py. This is best used when you want to do something all at once and filter your reads. Other approaches may be better when you are demultiplexing.","title":"batch_tater.py"},{"location":"fast5_fetcher/#usage","text":"Run on SGE using array jobs as a hacky way of doing multiprocessing. Also, helps check when things go wrong, and easy to relaunch failed jobs.","title":"Usage:"},{"location":"fast5_fetcher/#batchsge","text":"source ~/work/venv2714/bin/activate FILE=$(ls ./fast5/ | sed -n ${SGE_TASK_ID}p) BLAH=fast5/${FILE} mkdir ${TMPDIR}/fast5 time python batch_tater.py tater_master.txt ${BLAH} ${TMPDIR}/fast5/ echo size of files: 2 du -shc ${TMPDIR}/fast5/ 2 echo extraction complete! 2 echo Number of files: 2 ls ${TMPDIR}/fast5/ | wc -l 2 echo copying data... 2 tar -cf ${TMPDIR}/batch.${SGE_TASK_ID}.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 cp ${TMPDIR}/batch.${SGE_TASK_ID}.tar ./batched_fast5/","title":"batch.sge"},{"location":"fast5_fetcher/#create-cmd-and-launch","text":"CMD= qsub -cwd -V -pe smp 1 -N batch -S /bin/bash -t 1-10433 -tc 80 -l mem_requested=20G,h_vmem=20G,tmp_requested=200G ../batch.sge echo $CMD $CMD","title":"Create CMD and launch"},{"location":"files/","text":"Overview TODO: Do this for all tools, not just f5f There is one catch. Everything is written primarily for use with Linux . Due to MacOS running on Unix, so long as the GNU tools are installed (see below), there should be minimal issues running it. Windows 10 however may require more massaging to work with the new Linux integration. Getting Started Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure. File structures The file structure is not overly important, however it will modify some of the commands used in the examples. I have endeavoured to include a few diverse uses, starting from different file states, but of course, I can't think of everything, so if there is something you wish to accomplished with fast5_fetcher.py , but can't quite get it to work for you, let me know, and perhaps I can make it easier for you. 1. Raw structure (not preferred) This is the most basic structure, where all files are present in an accessible state. \u251c\u2500\u2500 huntsman.fastq \u251c\u2500\u2500 sequencing_summary.txt \u251c\u2500\u2500 huntsman_reads/ # Read folder \u2502 \u251c\u2500\u2500 0/ # individual folders containing ~4000 fast5s | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ... 2. Local basecalled structure This structure is the typical structure post local basecalling fastq and sequencing_summary files have been gzipped and the folders in the reads folder have been tarballed into one large file \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 huntsman_reads.tar # Tarballed read folder | # Tarball expanded |-- \u2502 \u251c\u2500\u2500 0/ # individual folders inside tarball | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ... 3. Parallel basecalled structure This structure is post massively parallel basecalling, and looks like multiples of the above structure. \u251c\u2500\u2500 fastq/ | \u251c\u2500\u2500 huntsman.1.fastq.gz | \u2514\u2500\u2500 huntsman.2.fastq.gz | \u2514\u2500\u2500 huntsman.3.fastq.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 logs/ | \u251c\u2500\u2500 sequencing_summary.1.txt.gz | \u2514\u2500\u2500 sequencing_summary.2.txt.gz | \u2514\u2500\u2500 sequencing_summary.3.txt.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 fast5/ | \u251c\u2500\u2500 1.tar | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... With this structure, combining the .fastq and sequencing_summary.txt.gz files is needed. Combine fastq.gz files for file in fastq/*.fastq.gz; do cat $file; done huntsman.fastq.gz Combine sequencing_summary.txt.gz files # create header zcat $(ls logs/sequencing_summary*.txt.gz | head -1) | head -1 sequencing_summary.txt # combine all files, skipping first line header for file in logs/sequencing_summary*.txt.gz; do zcat $file | tail -n +2; done sequencing_summary.txt gzip sequencing_summary.txt You should then have something like this: \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 fast5/ # fast5 folder | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... Inputs It takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz) 1. fastq, paf, or flat This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2. This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is just a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. See examples below for example test cases. 2. Sequencing summary The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not. 3. Building the index How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. Tarred is preferred. - Raw structure (not preferred) for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index - Local basecalled structure for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index - Parallel basecalled structure for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index","title":"Files and folders"},{"location":"files/#overview","text":"","title":"Overview"},{"location":"files/#todo-do-this-for-all-tools-not-just-f5f","text":"There is one catch. Everything is written primarily for use with Linux . Due to MacOS running on Unix, so long as the GNU tools are installed (see below), there should be minimal issues running it. Windows 10 however may require more massaging to work with the new Linux integration.","title":"TODO: Do this for all tools, not just f5f"},{"location":"files/#getting-started","text":"Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure.","title":"Getting Started"},{"location":"files/#file-structures","text":"The file structure is not overly important, however it will modify some of the commands used in the examples. I have endeavoured to include a few diverse uses, starting from different file states, but of course, I can't think of everything, so if there is something you wish to accomplished with fast5_fetcher.py , but can't quite get it to work for you, let me know, and perhaps I can make it easier for you.","title":"File structures"},{"location":"files/#1-raw-structure-not-preferred","text":"This is the most basic structure, where all files are present in an accessible state. \u251c\u2500\u2500 huntsman.fastq \u251c\u2500\u2500 sequencing_summary.txt \u251c\u2500\u2500 huntsman_reads/ # Read folder \u2502 \u251c\u2500\u2500 0/ # individual folders containing ~4000 fast5s | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ...","title":"1. Raw structure (not preferred)"},{"location":"files/#2-local-basecalled-structure","text":"This structure is the typical structure post local basecalling fastq and sequencing_summary files have been gzipped and the folders in the reads folder have been tarballed into one large file \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 huntsman_reads.tar # Tarballed read folder | # Tarball expanded |-- \u2502 \u251c\u2500\u2500 0/ # individual folders inside tarball | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ...","title":"2. Local basecalled structure"},{"location":"files/#3-parallel-basecalled-structure","text":"This structure is post massively parallel basecalling, and looks like multiples of the above structure. \u251c\u2500\u2500 fastq/ | \u251c\u2500\u2500 huntsman.1.fastq.gz | \u2514\u2500\u2500 huntsman.2.fastq.gz | \u2514\u2500\u2500 huntsman.3.fastq.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 logs/ | \u251c\u2500\u2500 sequencing_summary.1.txt.gz | \u2514\u2500\u2500 sequencing_summary.2.txt.gz | \u2514\u2500\u2500 sequencing_summary.3.txt.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 fast5/ | \u251c\u2500\u2500 1.tar | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... With this structure, combining the .fastq and sequencing_summary.txt.gz files is needed.","title":"3. Parallel basecalled structure"},{"location":"files/#combine-fastqgz-files","text":"for file in fastq/*.fastq.gz; do cat $file; done huntsman.fastq.gz","title":"Combine fastq.gz files"},{"location":"files/#combine-sequencing_summarytxtgz-files","text":"# create header zcat $(ls logs/sequencing_summary*.txt.gz | head -1) | head -1 sequencing_summary.txt # combine all files, skipping first line header for file in logs/sequencing_summary*.txt.gz; do zcat $file | tail -n +2; done sequencing_summary.txt gzip sequencing_summary.txt You should then have something like this: \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 fast5/ # fast5 folder | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ...","title":"Combine sequencing_summary.txt.gz files"},{"location":"files/#inputs","text":"It takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz)","title":"Inputs"},{"location":"files/#1-fastq-paf-or-flat","text":"This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2. This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is just a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. See examples below for example test cases.","title":"1. fastq, paf, or flat"},{"location":"files/#2-sequencing-summary","text":"The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not.","title":"2. Sequencing summary"},{"location":"files/#3-building-the-index","text":"How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. Tarred is preferred.","title":"3. Building the index"},{"location":"files/#-raw-structure-not-preferred","text":"for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index","title":"- Raw structure (not preferred)"},{"location":"files/#-local-basecalled-structure","text":"for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index","title":"- Local basecalled structure"},{"location":"files/#-parallel-basecalled-structure","text":"for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index","title":"- Parallel basecalled structure"},{"location":"install/","text":"Installation Requirements Following a self imposed guideline, most things written to handle nanopore data or bioinformatics in general, will use as little 3rd party libraries as possible, aiming for only core libraries, or have all included files in the package. In the case of fast5_fetcher.py and batch_tater.py , only core python libraries are used. So as long as Python 2.7+ is present, everything should work with no extra steps. (Python 3 compatibility is coming in the next big update) Install git clone https://github.com/Psy-Fer/SquiggleKit.git Quick start fast5_fetcher If using MacOS, and NOT using homebrew, install it here: homebrew installation instructions then install gnu-tar with: brew install gnu-tar Basic use on a local computer fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 SquigglePull All raw data: python SquigglePull.py -rv -p ~/data/test/reads/1/ -f all data.tsv Positional event data: python SquigglePull.py -ev -p ./test/ -t 50,150 -f pos1 data.tsv segmenter Stall identification python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv MotifSeq Nanopore adapter identification python sigtools.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model -t 120 -d 120 signals_adapters.tsv","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#requirements","text":"Following a self imposed guideline, most things written to handle nanopore data or bioinformatics in general, will use as little 3rd party libraries as possible, aiming for only core libraries, or have all included files in the package. In the case of fast5_fetcher.py and batch_tater.py , only core python libraries are used. So as long as Python 2.7+ is present, everything should work with no extra steps. (Python 3 compatibility is coming in the next big update)","title":"Requirements"},{"location":"install/#install","text":"git clone https://github.com/Psy-Fer/SquiggleKit.git","title":"Install"},{"location":"install/#quick-start","text":"","title":"Quick start"},{"location":"install/#fast5_fetcher","text":"If using MacOS, and NOT using homebrew, install it here: homebrew installation instructions then install gnu-tar with: brew install gnu-tar","title":"fast5_fetcher"},{"location":"install/#basic-use-on-a-local-computer","text":"fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5","title":"Basic use on a local computer"},{"location":"install/#squigglepull","text":"All raw data: python SquigglePull.py -rv -p ~/data/test/reads/1/ -f all data.tsv Positional event data: python SquigglePull.py -ev -p ./test/ -t 50,150 -f pos1 data.tsv","title":"SquigglePull"},{"location":"install/#segmenter","text":"Stall identification python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv","title":"segmenter"},{"location":"install/#motifseq","text":"Nanopore adapter identification python sigtools.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model -t 120 -d 120 signals_adapters.tsv","title":"MotifSeq"},{"location":"segmenter/","text":"Segmenter segmenter docs","title":"segmenter"},{"location":"segmenter/#segmenter","text":"segmenter docs","title":"Segmenter"}]}