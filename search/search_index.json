{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SquiggleKit Docs SquiggleKit is a toolkit for accessing and manipulating nanopore signal data. It has also been designed to act as an introduction to the data files produced by Oxford Nanopore Technology sequencing devices. Code: SquiggleKit . Publication: ...... Overview Tool Category Description Fast5_fetcher File management Fetches fast5 files given a filtered input list SquigglePull Signal extraction Extracts event or raw signal from data files SquigglePlot Signal visualisation Visualisation tool for signal data Segmenter Signal analysis Finds adapter stall, and homopolymer regions MotifSeq Signal analysis Finds nucleotide sequence motifs in signal, i.e.\u201cCtrl+F\u201d Getting help with SquiggleKit Happy to help and receive feedback. You can leave an issue on the github repo: SquiggleKit , @Psy_Fer_ on twitter, or shoot me an email at j.ferguson@garvan.org.au Created and maintained by James Ferguson from Genomic Technologies, Kinghorn Centre for Clinical Genomics, Garvan Institute , Sydney Australia.","title":"Home"},{"location":"#welcome-to-squigglekit-docs","text":"SquiggleKit is a toolkit for accessing and manipulating nanopore signal data. It has also been designed to act as an introduction to the data files produced by Oxford Nanopore Technology sequencing devices. Code: SquiggleKit . Publication: ......","title":"Welcome to SquiggleKit Docs"},{"location":"#overview","text":"Tool Category Description Fast5_fetcher File management Fetches fast5 files given a filtered input list SquigglePull Signal extraction Extracts event or raw signal from data files SquigglePlot Signal visualisation Visualisation tool for signal data Segmenter Signal analysis Finds adapter stall, and homopolymer regions MotifSeq Signal analysis Finds nucleotide sequence motifs in signal, i.e.\u201cCtrl+F\u201d","title":"Overview"},{"location":"#getting-help-with-squigglekit","text":"Happy to help and receive feedback. You can leave an issue on the github repo: SquiggleKit , @Psy_Fer_ on twitter, or shoot me an email at j.ferguson@garvan.org.au Created and maintained by James Ferguson from Genomic Technologies, Kinghorn Centre for Clinical Genomics, Garvan Institute , Sydney Australia.","title":"Getting help with SquiggleKit"},{"location":"MotifSeq/","text":"MotifSeq Background MotifSeq , the ctrl+f for signal, identifies raw signal traces that correspond to a given nucleotide sequence, such as an adapter, barcode or motif of interest. MotifSeq takes a query nucleotide sequence as input, converts it to a normalised signal trace using Scrappie, then performs a signal-level local alignment using a dynamic programming algorithm. MotifSeq outputs the location of a matching target in the raw signal with an associated distance value. Getting Started MotifSeq requires an input signal, either extracted from another read, or by using something such as Scrappie. Use the built in visualisation -v for help in parameter tuning. Instructions for use Nanopore adapter identification Building an adapter model: scrappie squiggle adapter.fa adapter.model Identify stalls in signal using segmenter: python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv Identifying nanopore adapters in signal up stream of identified stalls: python MotifSeq.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model signals_adapters.tsv Full usage usage: MotifSeq.py [-h] [-f F5F | -p F5_PATH | -s SIGNAL] [-a ADAPT] [-m MODELS] [--segs SEGS] [-b BAITS] [-t DTW_THRESH] [-d MOTIF_DIST] [-v] [-scale_hi SCALE_HI] [-scale_low SCALE_LOW] MotifSeq - the Ctrl+f for signal. Signal-level local alignment of sequence motifs. optional arguments: -h, --help show this help message and exit -f F5F, --f5f F5F File list of fast5 paths -p F5_PATH, --f5_path F5_PATH Fast5 top dir -s SIGNAL, --signal SIGNAL Extracted signal file from SquigglePull -a ADAPT, --adapt ADAPT Adapter model file -m MODEL, --model MODEL Query model file --segs SEGS segmenter file -b BAITS, --baits BAITS signal bait file -t DTW_THRESH, --dtw_thresh DTW_THRESH DTW distance threshold for match -d MOTIF_DIST, --motif_dist MOTIF_DIST max distance of adapter from start of signal -v, --view view each output -scale_hi SCALE_HI, --scale_hi SCALE_HI Upper limit for signal outlier scaling -scale_low SCALE_LOW, --scale_low SCALE_LOW Lower limit for signal outlier scaling","title":"MotifSeq"},{"location":"MotifSeq/#motifseq","text":"","title":"MotifSeq"},{"location":"MotifSeq/#background","text":"MotifSeq , the ctrl+f for signal, identifies raw signal traces that correspond to a given nucleotide sequence, such as an adapter, barcode or motif of interest. MotifSeq takes a query nucleotide sequence as input, converts it to a normalised signal trace using Scrappie, then performs a signal-level local alignment using a dynamic programming algorithm. MotifSeq outputs the location of a matching target in the raw signal with an associated distance value.","title":"Background"},{"location":"MotifSeq/#getting-started","text":"MotifSeq requires an input signal, either extracted from another read, or by using something such as Scrappie. Use the built in visualisation -v for help in parameter tuning.","title":"Getting Started"},{"location":"MotifSeq/#instructions-for-use","text":"Nanopore adapter identification Building an adapter model: scrappie squiggle adapter.fa adapter.model Identify stalls in signal using segmenter: python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv Identifying nanopore adapters in signal up stream of identified stalls: python MotifSeq.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model signals_adapters.tsv","title":"Instructions for use"},{"location":"MotifSeq/#full-usage","text":"usage: MotifSeq.py [-h] [-f F5F | -p F5_PATH | -s SIGNAL] [-a ADAPT] [-m MODELS] [--segs SEGS] [-b BAITS] [-t DTW_THRESH] [-d MOTIF_DIST] [-v] [-scale_hi SCALE_HI] [-scale_low SCALE_LOW] MotifSeq - the Ctrl+f for signal. Signal-level local alignment of sequence motifs. optional arguments: -h, --help show this help message and exit -f F5F, --f5f F5F File list of fast5 paths -p F5_PATH, --f5_path F5_PATH Fast5 top dir -s SIGNAL, --signal SIGNAL Extracted signal file from SquigglePull -a ADAPT, --adapt ADAPT Adapter model file -m MODEL, --model MODEL Query model file --segs SEGS segmenter file -b BAITS, --baits BAITS signal bait file -t DTW_THRESH, --dtw_thresh DTW_THRESH DTW distance threshold for match -d MOTIF_DIST, --motif_dist MOTIF_DIST max distance of adapter from start of signal -v, --view view each output -scale_hi SCALE_HI, --scale_hi SCALE_HI Upper limit for signal outlier scaling -scale_low SCALE_LOW, --scale_low SCALE_LOW Lower limit for signal outlier scaling","title":"Full usage"},{"location":"SquigglePlot/","text":"SquigglePlot Background When working with signal level data, generating a simple visualisation to help understand the structural breakdown of a nanopore read, or to simply creating a nice figure, and easy to use, multiple format, plotting script is needed. SquigglePlot allows for visualising sections of the signal, as well as implements some simple scaling to clean up hi/low noise, save in multiple formats (pdf, png, etc), and set the DPI of the save file for publications or posters. Getting Started SquigglePlot acts as a wrapper for matplotlib in python. Extract your signal with SquigglePull or point it at some fast5 files. Instructions for use Provide a top folder path -p , a signal file from SquigglePull.py -s , or an individual fast5 file -i , and SquigglePlot will plot the raw signal for you. Quick start Individual File full signal python SquigglePlot.py -i ~/data/test.fast5 Plot first 2000 data points of each read from signal file and save at 300dpi pdf * python SquigglePlot.py -s signals.tsv.gz --plot_colour teal -n 2000 --dpi 300 --no_show o--save test.pdf --save_path ./test/plots/ Plot all from top folder in green python SquigglePlot.py -p ~/data/ --plot_colour -g Plot between 2 values in pink python SquigglePlot.py -i ~/data/test.fast5 --plot_colour pink -n 400,1600 Full usage usage: SquigglePlot.py [-h] [-f F5F | -p F5_PATH | -s SIGNAL | -i IND] [--head] [-n NUM] [--scale_hi SCALE_HI] [--scale_low SCALE_LOW] [--plot_colour PLOT_COLOUR] [--save SAVE] [--save_path SAVE_PATH] [--no_show] [--dpi DPI] script name - script description optional arguments: -h, --help show this help message and exit -f F5F, --f5f F5F File list of fast5 paths -p F5_PATH, --f5_path F5_PATH Fast5 top dir -s SIGNAL, --signal SIGNAL Extracted signal file from SquigglePull -i IND, --ind IND Individual fast5 file --head Header present in signal or flat file -n NUM, --Num NUM Section of signal to look at - -n 2000 or -n 100,1500 --scale_hi SCALE_HI Upper limit for signal outlier scaling --scale_low SCALE_LOW Lower limit for signal outlier scaling --plot_colour PLOT_COLOUR Colour of signal plot, takes any pyplot entry: k,r,b,g,red,blue,etc... --save SAVE Save file readname_saveArg.pdf --save saveArg.pdf, use png, etc for other file types --save_path SAVE_PATH Save filepath --no_show Do not show plot (used for saving many) --dpi DPI Change DPI for publication figs, eg: --dpi 300","title":"SquigglePlot"},{"location":"SquigglePlot/#squiggleplot","text":"","title":"SquigglePlot"},{"location":"SquigglePlot/#background","text":"When working with signal level data, generating a simple visualisation to help understand the structural breakdown of a nanopore read, or to simply creating a nice figure, and easy to use, multiple format, plotting script is needed. SquigglePlot allows for visualising sections of the signal, as well as implements some simple scaling to clean up hi/low noise, save in multiple formats (pdf, png, etc), and set the DPI of the save file for publications or posters.","title":"Background"},{"location":"SquigglePlot/#getting-started","text":"SquigglePlot acts as a wrapper for matplotlib in python. Extract your signal with SquigglePull or point it at some fast5 files.","title":"Getting Started"},{"location":"SquigglePlot/#instructions-for-use","text":"Provide a top folder path -p , a signal file from SquigglePull.py -s , or an individual fast5 file -i , and SquigglePlot will plot the raw signal for you.","title":"Instructions for use"},{"location":"SquigglePlot/#quick-start","text":"Individual File full signal python SquigglePlot.py -i ~/data/test.fast5 Plot first 2000 data points of each read from signal file and save at 300dpi pdf * python SquigglePlot.py -s signals.tsv.gz --plot_colour teal -n 2000 --dpi 300 --no_show o--save test.pdf --save_path ./test/plots/ Plot all from top folder in green python SquigglePlot.py -p ~/data/ --plot_colour -g Plot between 2 values in pink python SquigglePlot.py -i ~/data/test.fast5 --plot_colour pink -n 400,1600","title":"Quick start"},{"location":"SquigglePlot/#full-usage","text":"usage: SquigglePlot.py [-h] [-f F5F | -p F5_PATH | -s SIGNAL | -i IND] [--head] [-n NUM] [--scale_hi SCALE_HI] [--scale_low SCALE_LOW] [--plot_colour PLOT_COLOUR] [--save SAVE] [--save_path SAVE_PATH] [--no_show] [--dpi DPI] script name - script description optional arguments: -h, --help show this help message and exit -f F5F, --f5f F5F File list of fast5 paths -p F5_PATH, --f5_path F5_PATH Fast5 top dir -s SIGNAL, --signal SIGNAL Extracted signal file from SquigglePull -i IND, --ind IND Individual fast5 file --head Header present in signal or flat file -n NUM, --Num NUM Section of signal to look at - -n 2000 or -n 100,1500 --scale_hi SCALE_HI Upper limit for signal outlier scaling --scale_low SCALE_LOW Lower limit for signal outlier scaling --plot_colour PLOT_COLOUR Colour of signal plot, takes any pyplot entry: k,r,b,g,red,blue,etc... --save SAVE Save file readname_saveArg.pdf --save saveArg.pdf, use png, etc for other file types --save_path SAVE_PATH Save filepath --no_show Do not show plot (used for saving many) --dpi DPI Change DPI for publication figs, eg: --dpi 300","title":"Full usage"},{"location":"SquigglePull/","text":"SquigglePull Background Up until early 2019, Oxford Nanopore sequencing devices stored the raw current measurements and associated metadata in a single Hierarchical Data Format (HDF5) format file, called a fast5 file. After early 2019 devices were updated to produce multi-fast5 files, which contained multiple reads, usually 4000, in a single file. Processing fast5 files has been troublesome for a number of reasons: Unfamiliar format to many people. Need 3rd party library to open and read (h5py, pytables). libraries are not thread safe and locks files. Don't compress as well when single files. (better in multi-fast5) SquigglePull outputs a single tab separated value (.tsv) file where each row contains a single signal and read selected metadata. Current format is designed as an example of producing a more accessible file format for those wishing to get started with nanopore signal data. Columns and data inclusion is subject to change depending on use case. Getting Started SquigglePull can extract both raw current measurements as well as event data. It also has some basic arguments and code scaffolding extraction profiles. This allows the user to implement their analysis methods into the extraction protocol for integration with pipelines, or just quick data trimming. For example using the form -f pos1 , and the tarting -t 20,110 , only the signal values between 20 and 110 will be extracted. Inputs SquigglePull takes 3 mandatory arguments path - Top directory of fast5 files form - Format of targeting information (default: all) raw/event - Raw signal or event data Instructions for use Simply point SquigglePull to a top directory containing fast5 files, and the signal will be extracted to STDOUT Quick start Extract all raw signal python SquigglePull.py -r -p test/R9_raw_data/ data.tsv Extract events between position 20 and 210 python SquigglePull.py -e -p test/R9_event_data/ -t 20,110 -f pos1 data.tsv Full usage usage: SquigglePull.py [-h] [-p PATH] [-t TARGET] [-f {pos1,all}] [-r | -e] [-v] [-s] SquigglePull - extraction of raw/event signal from Oxford Nanopore fast5 files optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Top directory path of fast5 files -t TARGET, --target TARGET Target information as comma delimited string structured by format type -f {pos1,all}, --form {pos1,all} Format of target information -r, --raw Target raw signal -e, --event Target event signal -v, --verbose Engage higher output verbosity -s, --scale Scale signal output for comparison","title":"SquigglePull"},{"location":"SquigglePull/#squigglepull","text":"","title":"SquigglePull"},{"location":"SquigglePull/#background","text":"Up until early 2019, Oxford Nanopore sequencing devices stored the raw current measurements and associated metadata in a single Hierarchical Data Format (HDF5) format file, called a fast5 file. After early 2019 devices were updated to produce multi-fast5 files, which contained multiple reads, usually 4000, in a single file. Processing fast5 files has been troublesome for a number of reasons: Unfamiliar format to many people. Need 3rd party library to open and read (h5py, pytables). libraries are not thread safe and locks files. Don't compress as well when single files. (better in multi-fast5) SquigglePull outputs a single tab separated value (.tsv) file where each row contains a single signal and read selected metadata. Current format is designed as an example of producing a more accessible file format for those wishing to get started with nanopore signal data. Columns and data inclusion is subject to change depending on use case.","title":"Background"},{"location":"SquigglePull/#getting-started","text":"SquigglePull can extract both raw current measurements as well as event data. It also has some basic arguments and code scaffolding extraction profiles. This allows the user to implement their analysis methods into the extraction protocol for integration with pipelines, or just quick data trimming. For example using the form -f pos1 , and the tarting -t 20,110 , only the signal values between 20 and 110 will be extracted.","title":"Getting Started"},{"location":"SquigglePull/#inputs","text":"SquigglePull takes 3 mandatory arguments path - Top directory of fast5 files form - Format of targeting information (default: all) raw/event - Raw signal or event data","title":"Inputs"},{"location":"SquigglePull/#instructions-for-use","text":"Simply point SquigglePull to a top directory containing fast5 files, and the signal will be extracted to STDOUT","title":"Instructions for use"},{"location":"SquigglePull/#quick-start","text":"Extract all raw signal python SquigglePull.py -r -p test/R9_raw_data/ data.tsv Extract events between position 20 and 210 python SquigglePull.py -e -p test/R9_event_data/ -t 20,110 -f pos1 data.tsv","title":"Quick start"},{"location":"SquigglePull/#full-usage","text":"usage: SquigglePull.py [-h] [-p PATH] [-t TARGET] [-f {pos1,all}] [-r | -e] [-v] [-s] SquigglePull - extraction of raw/event signal from Oxford Nanopore fast5 files optional arguments: -h, --help show this help message and exit -p PATH, --path PATH Top directory path of fast5 files -t TARGET, --target TARGET Target information as comma delimited string structured by format type -f {pos1,all}, --form {pos1,all} Format of target information -r, --raw Target raw signal -e, --event Target event signal -v, --verbose Engage higher output verbosity -s, --scale Scale signal output for comparison","title":"Full usage"},{"location":"examples/","text":"Examples The tools can be used in a variety of ways using different wrapping techniques. I have provided a few examples here. Barcode experiment partitioning with fast5_fetcher Fast5 Fetcher was originally built to work with Sun Grid Engine (SGE), exploiting the heck out of array jobs. Although it can work locally and on untarred file structures, when operating on multiple sequencing experiments, with file structures scattered across a file system, is when fast5 fetcher starts to make a difference. SGE examples After creating the fastq/paf/flat, sequencing_summary, and index files, create an SGE file. Note the use of ${SGE_TASK_ID} to use the array job as the pointer to a particular file After barcode demultiplexing Given a similar structure and naming convention, it is possible to group the fast5 files by barcode in the following manner. \u251c\u2500\u2500 BC_1.fastq.gz # Barcode 1 \u251c\u2500\u2500 BC_2.fastq.gz # Barcode 2 \u251c\u2500\u2500 BC_3.fastq.gz # ... \u251c\u2500\u2500 BC_4.fastq.gz \u251c\u2500\u2500 BC_5.fastq.gz \u251c\u2500\u2500 BC_6.fastq.gz \u251c\u2500\u2500 BC_7.fastq.gz \u251c\u2500\u2500 BC_8.fastq.gz \u251c\u2500\u2500 BC_9.fastq.gz \u251c\u2500\u2500 BC_10.fastq.gz \u251c\u2500\u2500 BC_11.fastq.gz \u251c\u2500\u2500 BC_12.fastq.gz \u251c\u2500\u2500 unclassified.fastq.gz # unclassified reads (skipped by fast5_fetcher in this example, rename BC_13 to simple fold it into the example) \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 barcoded.index.gz # index file containing fast5 file paths \u251c\u2500\u2500 fast5/ # fast5 folder, unsorted | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... fetch.sge # activate virtual python environment # most HPC will use something like module load source ~/work/venv2714/bin/activate # Creaete output directory to take advantage of NVME drives on cluster local mkdir ${TMPDIR}/fast5 # Run fast_fetcher on each barcode after demultiplexing time python fast5_fetcher.py -r ./BC_${SGE_TASK_ID}.fastq.gz -s sequencing_summary.txt.gz -i barcoded.index.gz -o ${TMPDIR}/fast5/ # tarball the extracted reads into a single tar file # Can also split the reads into groups of ~4000 if needed tar -cf ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 # Copy from HPC drives to working dir. cp ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar ./ Create CMD and launch # current working dir, with 1 CPU, array jobs 1 to 12 # Modify memory settings as required CMD= qsub -cwd -V -pe smp 1 -N F5F -S /bin/bash -t 1-12 -l mem_requested=20G,h_vmem=20G,tmp_requested=500G ./fetch.sge echo $CMD $CMD Raw signal extraction for analysis python SquigglePull.py -r -p test/R9_raw_data/ data.tsv Stall detection in raw data python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv Nanopore Adapter detection in raw data scrappie squiggle adapter.fa adapter.model python MotifSeq.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model signals_adapters.tsv Pipelines # activate virtual python environment # most HPC will use something like module load F5F=SquiggleKit/fast5_fetcher.py SPULL=SquiggleKit/SquigglePull.py # Filtered paf file PAF=full_length_60_R2_154_2.paf SEQ_SUM=LLAAB035083_sequencing_summary.txt INDEX=LLAAB035083_fast5.index # Fastq for filtering FQ=LLAAB035083.fastq.gz TMP_STORE=${TMPDIR}/fast5 source ~/venv2714/bin/activate # Create output directory to take advantage of NVME drives on cluster local mkdir ${TMP_STORE} echo fetching fast5 files... 2 # Run fast_fetcher on sample after filtering time python ${F5F} -p ${PAF} -s ${SEQ_SUM} -i ${INDEX} -o ${TMP_STORE} -t -x R2_154_2 -l ${FQ} echo extracting squiggles... 2 # now extract the signals from them with SquigglePull time python ${SPULL} -rv -p ${TMP_STORE} -f all ${TMPDIR}/squigs_R2_154_2.tsv echo copying data... 2 # Copy from HPC drives to working dir. cp ${TMPDIR}/*R2_154_2* ./ echo done! 2","title":"Examples"},{"location":"examples/#examples","text":"The tools can be used in a variety of ways using different wrapping techniques. I have provided a few examples here.","title":"Examples"},{"location":"examples/#barcode-experiment-partitioning-with-fast5_fetcher","text":"Fast5 Fetcher was originally built to work with Sun Grid Engine (SGE), exploiting the heck out of array jobs. Although it can work locally and on untarred file structures, when operating on multiple sequencing experiments, with file structures scattered across a file system, is when fast5 fetcher starts to make a difference.","title":"Barcode experiment partitioning with fast5_fetcher"},{"location":"examples/#sge-examples","text":"After creating the fastq/paf/flat, sequencing_summary, and index files, create an SGE file. Note the use of ${SGE_TASK_ID} to use the array job as the pointer to a particular file","title":"SGE examples"},{"location":"examples/#after-barcode-demultiplexing","text":"Given a similar structure and naming convention, it is possible to group the fast5 files by barcode in the following manner. \u251c\u2500\u2500 BC_1.fastq.gz # Barcode 1 \u251c\u2500\u2500 BC_2.fastq.gz # Barcode 2 \u251c\u2500\u2500 BC_3.fastq.gz # ... \u251c\u2500\u2500 BC_4.fastq.gz \u251c\u2500\u2500 BC_5.fastq.gz \u251c\u2500\u2500 BC_6.fastq.gz \u251c\u2500\u2500 BC_7.fastq.gz \u251c\u2500\u2500 BC_8.fastq.gz \u251c\u2500\u2500 BC_9.fastq.gz \u251c\u2500\u2500 BC_10.fastq.gz \u251c\u2500\u2500 BC_11.fastq.gz \u251c\u2500\u2500 BC_12.fastq.gz \u251c\u2500\u2500 unclassified.fastq.gz # unclassified reads (skipped by fast5_fetcher in this example, rename BC_13 to simple fold it into the example) \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 barcoded.index.gz # index file containing fast5 file paths \u251c\u2500\u2500 fast5/ # fast5 folder, unsorted | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ...","title":"After barcode demultiplexing"},{"location":"examples/#fetchsge","text":"# activate virtual python environment # most HPC will use something like module load source ~/work/venv2714/bin/activate # Creaete output directory to take advantage of NVME drives on cluster local mkdir ${TMPDIR}/fast5 # Run fast_fetcher on each barcode after demultiplexing time python fast5_fetcher.py -r ./BC_${SGE_TASK_ID}.fastq.gz -s sequencing_summary.txt.gz -i barcoded.index.gz -o ${TMPDIR}/fast5/ # tarball the extracted reads into a single tar file # Can also split the reads into groups of ~4000 if needed tar -cf ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 # Copy from HPC drives to working dir. cp ${TMPDIR}/BC_${SGE_TASK_ID}_fast5.tar ./","title":"fetch.sge"},{"location":"examples/#create-cmd-and-launch","text":"# current working dir, with 1 CPU, array jobs 1 to 12 # Modify memory settings as required CMD= qsub -cwd -V -pe smp 1 -N F5F -S /bin/bash -t 1-12 -l mem_requested=20G,h_vmem=20G,tmp_requested=500G ./fetch.sge echo $CMD $CMD","title":"Create CMD and launch"},{"location":"examples/#raw-signal-extraction-for-analysis","text":"python SquigglePull.py -r -p test/R9_raw_data/ data.tsv","title":"Raw signal extraction for analysis"},{"location":"examples/#stall-detection-in-raw-data","text":"python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv","title":"Stall detection in raw data"},{"location":"examples/#nanopore-adapter-detection-in-raw-data","text":"scrappie squiggle adapter.fa adapter.model python MotifSeq.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model signals_adapters.tsv","title":"Nanopore Adapter detection in raw data"},{"location":"examples/#pipelines","text":"# activate virtual python environment # most HPC will use something like module load F5F=SquiggleKit/fast5_fetcher.py SPULL=SquiggleKit/SquigglePull.py # Filtered paf file PAF=full_length_60_R2_154_2.paf SEQ_SUM=LLAAB035083_sequencing_summary.txt INDEX=LLAAB035083_fast5.index # Fastq for filtering FQ=LLAAB035083.fastq.gz TMP_STORE=${TMPDIR}/fast5 source ~/venv2714/bin/activate # Create output directory to take advantage of NVME drives on cluster local mkdir ${TMP_STORE} echo fetching fast5 files... 2 # Run fast_fetcher on sample after filtering time python ${F5F} -p ${PAF} -s ${SEQ_SUM} -i ${INDEX} -o ${TMP_STORE} -t -x R2_154_2 -l ${FQ} echo extracting squiggles... 2 # now extract the signals from them with SquigglePull time python ${SPULL} -rv -p ${TMP_STORE} -f all ${TMPDIR}/squigs_R2_154_2.tsv echo copying data... 2 # Copy from HPC drives to working dir. cp ${TMPDIR}/*R2_154_2* ./ echo done! 2","title":"Pipelines"},{"location":"fast5_fetcher/","text":"fast5_fetcher Doing the heavy lifting for you. fast5_fetcher is a tool for fetching nanopore fast5 files to save time and simplify downstream analysis. Background Reducing the number of fast5 files per folder in a single experiment was a welcomed addition to MinKnow. However this also made it rather useful for manual basecalling on a cluster, using array jobs, where each folder is basecalled individually, producing its own sequencing_summary.txt , reads.fastq , and reads folder containing the newly basecalled fast5s. Taring those fast5 files up into a single file was needed to keep the sys admins at bay, complaining about our millions of individual files on their drives. This meant, whenever there was a need to use the fast5 files from an experiment, or many experiments, unpacking the fast5 files was a significant hurdle both in time and disk space. fast5_fetcher was built to address this bottleneck. By building an index file of the tarballs, and using the sequencing_summary.txt file to match readIDs with fast5 filenames, only the fast5 files you need can be extracted, either temporarily in a pipeline, or permanently, reducing space and simplifying downstream work flows. Getting Started Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure. File structures See Files and folders section Inputs It takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz) 1. fastq, paf, or flat This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2. This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is just a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. See examples below for example test cases. 2. Sequencing summary The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not. 3. Building the index How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. Tarred is preferred. - Raw structure (not preferred) for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index - Local basecalled structure for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index - Parallel basecalled structure for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index Instructions for use If using MacOS, and NOT using homebrew, install it here: https://brew.sh/ then install gnu-tar with: brew install gnu-tar Quick start Basic use on a local computer fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 See examples below for use on an HPC using SGE fast5_fetcher.py Full usage usage: fast5_fetcher.py [-h] [-q FASTQ | -p PAF | -f FLAT] [--OSystem OSYSTEM] [-s SEQ_SUM] [-i INDEX] [-o OUTPUT] [-t] [-l TRIM_LIST] [-x PREFIX] [-z] fast_fetcher - extraction of specific nanopore fast5 files optional arguments: -h, --help show this help message and exit -q FASTQ, --fastq FASTQ fastq.gz for read ids -p PAF, --paf PAF paf alignment file for read ids -f FLAT, --flat FLAT flat file of read ids --OSystem OSYSTEM running operating system - leave default unless doing odd stuff -s SEQ_SUM, --seq_sum SEQ_SUM sequencing_summary.txt.gz file -i INDEX, --index INDEX index.gz file mapping fast5 files in tar archives -o OUTPUT, --output OUTPUT output directory for extracted fast5s -t, --trim trim files as if standalone experiment, (fq, SS) -l TRIM_LIST, --trim_list TRIM_LIST list of file names to trim, comma separated. fastq only needed for -p and -f modes -x PREFIX, --prefix PREFIX trim file prefix, eg: barcode_01, output: barcode_01.fastq, barcode_01_seq_sum.txt -z, --pppp Print out tar commands in batches for further processing Trimming fastq and sequencing_summary files By using the -t, --trim option, each barcode will also have its own sequencing_summary file for downstream analysis. This is particularly useful if each barcode is a different sample or experiment, as the output is as if it was it's own individual flowcell. This method can also trim fastq, and sequencing_summary files when using the paf or flat methods. By using the prefix option, you can label the output names, otherwise generic defaults will be used. batch_tater.py Potato scripting engaged This is designed to run on the output files from fast5_fetcher.py using option -z . This writes out file lists for each tarball that contains reads you want to process. Then batch_tater.py can read those files, to open the individual tar files, and extract the files, meaning the file is only opened once. A recent test using the -z option on ~2.2Tb of data, across ~11/27 million files took about 10min (1CPU) to write and organise the file lists with fast5_fetch.py, and about 20s per array job to extract and repackage with batch_tater.py. This is best used when you want to do something all at once and filter your reads. Other approaches may be better when you are demultiplexing. Usage: Run on SGE using array jobs as a hacky way of doing multiprocessing. Also, helps check when things go wrong, and easy to relaunch failed jobs. batch.sge source ~/work/venv2714/bin/activate FILE=$(ls ./fast5/ | sed -n ${SGE_TASK_ID}p) BLAH=fast5/${FILE} mkdir ${TMPDIR}/fast5 time python batch_tater.py tater_master.txt ${BLAH} ${TMPDIR}/fast5/ echo size of files: 2 du -shc ${TMPDIR}/fast5/ 2 echo extraction complete! 2 echo Number of files: 2 ls ${TMPDIR}/fast5/ | wc -l 2 echo copying data... 2 tar -cf ${TMPDIR}/batch.${SGE_TASK_ID}.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 cp ${TMPDIR}/batch.${SGE_TASK_ID}.tar ./batched_fast5/ Create CMD and launch CMD= qsub -cwd -V -pe smp 1 -N batch -S /bin/bash -t 1-10433 -tc 80 -l mem_requested=20G,h_vmem=20G,tmp_requested=200G ../batch.sge echo $CMD $CMD","title":"fast5_fetcher"},{"location":"fast5_fetcher/#fast5_fetcher","text":"","title":"fast5_fetcher"},{"location":"fast5_fetcher/#doing-the-heavy-lifting-for-you","text":"fast5_fetcher is a tool for fetching nanopore fast5 files to save time and simplify downstream analysis.","title":"Doing the heavy lifting for you."},{"location":"fast5_fetcher/#background","text":"Reducing the number of fast5 files per folder in a single experiment was a welcomed addition to MinKnow. However this also made it rather useful for manual basecalling on a cluster, using array jobs, where each folder is basecalled individually, producing its own sequencing_summary.txt , reads.fastq , and reads folder containing the newly basecalled fast5s. Taring those fast5 files up into a single file was needed to keep the sys admins at bay, complaining about our millions of individual files on their drives. This meant, whenever there was a need to use the fast5 files from an experiment, or many experiments, unpacking the fast5 files was a significant hurdle both in time and disk space. fast5_fetcher was built to address this bottleneck. By building an index file of the tarballs, and using the sequencing_summary.txt file to match readIDs with fast5 filenames, only the fast5 files you need can be extracted, either temporarily in a pipeline, or permanently, reducing space and simplifying downstream work flows.","title":"Background"},{"location":"fast5_fetcher/#getting-started","text":"Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure.","title":"Getting Started"},{"location":"fast5_fetcher/#file-structures","text":"See Files and folders section","title":"File structures"},{"location":"fast5_fetcher/#inputs","text":"It takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz)","title":"Inputs"},{"location":"fast5_fetcher/#1-fastq-paf-or-flat","text":"This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2. This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is just a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. See examples below for example test cases.","title":"1. fastq, paf, or flat"},{"location":"fast5_fetcher/#2-sequencing-summary","text":"The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not.","title":"2. Sequencing summary"},{"location":"fast5_fetcher/#3-building-the-index","text":"How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. Tarred is preferred.","title":"3. Building the index"},{"location":"fast5_fetcher/#-raw-structure-not-preferred","text":"for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index","title":"- Raw structure (not preferred)"},{"location":"fast5_fetcher/#-local-basecalled-structure","text":"for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index","title":"- Local basecalled structure"},{"location":"fast5_fetcher/#-parallel-basecalled-structure","text":"for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index","title":"- Parallel basecalled structure"},{"location":"fast5_fetcher/#instructions-for-use","text":"If using MacOS, and NOT using homebrew, install it here: https://brew.sh/ then install gnu-tar with: brew install gnu-tar","title":"Instructions for use"},{"location":"fast5_fetcher/#quick-start","text":"Basic use on a local computer fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 See examples below for use on an HPC using SGE","title":"Quick start"},{"location":"fast5_fetcher/#fast5_fetcherpy","text":"","title":"fast5_fetcher.py"},{"location":"fast5_fetcher/#full-usage","text":"usage: fast5_fetcher.py [-h] [-q FASTQ | -p PAF | -f FLAT] [--OSystem OSYSTEM] [-s SEQ_SUM] [-i INDEX] [-o OUTPUT] [-t] [-l TRIM_LIST] [-x PREFIX] [-z] fast_fetcher - extraction of specific nanopore fast5 files optional arguments: -h, --help show this help message and exit -q FASTQ, --fastq FASTQ fastq.gz for read ids -p PAF, --paf PAF paf alignment file for read ids -f FLAT, --flat FLAT flat file of read ids --OSystem OSYSTEM running operating system - leave default unless doing odd stuff -s SEQ_SUM, --seq_sum SEQ_SUM sequencing_summary.txt.gz file -i INDEX, --index INDEX index.gz file mapping fast5 files in tar archives -o OUTPUT, --output OUTPUT output directory for extracted fast5s -t, --trim trim files as if standalone experiment, (fq, SS) -l TRIM_LIST, --trim_list TRIM_LIST list of file names to trim, comma separated. fastq only needed for -p and -f modes -x PREFIX, --prefix PREFIX trim file prefix, eg: barcode_01, output: barcode_01.fastq, barcode_01_seq_sum.txt -z, --pppp Print out tar commands in batches for further processing","title":"Full usage"},{"location":"fast5_fetcher/#trimming-fastq-and-sequencing_summary-files","text":"By using the -t, --trim option, each barcode will also have its own sequencing_summary file for downstream analysis. This is particularly useful if each barcode is a different sample or experiment, as the output is as if it was it's own individual flowcell. This method can also trim fastq, and sequencing_summary files when using the paf or flat methods. By using the prefix option, you can label the output names, otherwise generic defaults will be used.","title":"Trimming fastq and sequencing_summary files"},{"location":"fast5_fetcher/#batch_taterpy","text":"Potato scripting engaged This is designed to run on the output files from fast5_fetcher.py using option -z . This writes out file lists for each tarball that contains reads you want to process. Then batch_tater.py can read those files, to open the individual tar files, and extract the files, meaning the file is only opened once. A recent test using the -z option on ~2.2Tb of data, across ~11/27 million files took about 10min (1CPU) to write and organise the file lists with fast5_fetch.py, and about 20s per array job to extract and repackage with batch_tater.py. This is best used when you want to do something all at once and filter your reads. Other approaches may be better when you are demultiplexing.","title":"batch_tater.py"},{"location":"fast5_fetcher/#usage","text":"Run on SGE using array jobs as a hacky way of doing multiprocessing. Also, helps check when things go wrong, and easy to relaunch failed jobs.","title":"Usage:"},{"location":"fast5_fetcher/#batchsge","text":"source ~/work/venv2714/bin/activate FILE=$(ls ./fast5/ | sed -n ${SGE_TASK_ID}p) BLAH=fast5/${FILE} mkdir ${TMPDIR}/fast5 time python batch_tater.py tater_master.txt ${BLAH} ${TMPDIR}/fast5/ echo size of files: 2 du -shc ${TMPDIR}/fast5/ 2 echo extraction complete! 2 echo Number of files: 2 ls ${TMPDIR}/fast5/ | wc -l 2 echo copying data... 2 tar -cf ${TMPDIR}/batch.${SGE_TASK_ID}.tar --transform='s/.*\\///' ${TMPDIR}/fast5/*.fast5 cp ${TMPDIR}/batch.${SGE_TASK_ID}.tar ./batched_fast5/","title":"batch.sge"},{"location":"fast5_fetcher/#create-cmd-and-launch","text":"CMD= qsub -cwd -V -pe smp 1 -N batch -S /bin/bash -t 1-10433 -tc 80 -l mem_requested=20G,h_vmem=20G,tmp_requested=200G ../batch.sge echo $CMD $CMD","title":"Create CMD and launch"},{"location":"files/","text":"Overview Most files and formats are straightforward. The only tricky starting point is fast5_fetcher.py . This becomes even more important if using something like batch_tater.py with SGE to significantly reduce the time taken to run. Once processing more than 50k files, the batch_tater.py method becomes very attractive. Getting Started Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure. File structures I have endeavoured to include a few diverse uses, starting from different file states, but of course, I can't think of everything, so if there is something you wish to accomplished with fast5_fetcher.py , but can't quite get it to work for you, let me know, and we can try to work it out. 1. Raw structure This is the most basic structure, where all files are present in an accessible state. \u251c\u2500\u2500 huntsman.fastq \u251c\u2500\u2500 sequencing_summary.txt \u251c\u2500\u2500 huntsman_reads/ # Read folder \u2502 \u251c\u2500\u2500 0/ # individual folders containing ~4000 fast5s | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ... 2. Local basecalled structure This structure is the typical structure post local basecalling fastq and sequencing_summary files have been gzipped and the folders in the reads folder have been tarballed into one large file \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 huntsman_reads.tar # Tarballed read folder | # Tarball expanded |-- \u2502 \u251c\u2500\u2500 0/ # individual folders inside tarball | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ... 3. Parallel basecalled structure This structure is post massively parallel basecalling, and looks like multiples of the above structure. \u251c\u2500\u2500 fastq/ | \u251c\u2500\u2500 huntsman.1.fastq.gz | \u2514\u2500\u2500 huntsman.2.fastq.gz | \u2514\u2500\u2500 huntsman.3.fastq.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 logs/ | \u251c\u2500\u2500 sequencing_summary.1.txt.gz | \u2514\u2500\u2500 sequencing_summary.2.txt.gz | \u2514\u2500\u2500 sequencing_summary.3.txt.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 fast5/ | \u251c\u2500\u2500 1.tar | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... With this structure, combining the .fastq and sequencing_summary.txt.gz files is needed. Combine fastq.gz files for file in fastq/*.fastq.gz; do cat $file; done huntsman.fastq.gz Combine sequencing_summary.txt.gz files # create header zcat $(ls logs/sequencing_summary*.txt.gz | head -1) | head -1 sequencing_summary.txt # combine all files, skipping first line header for file in logs/sequencing_summary*.txt.gz; do zcat $file | tail -n +2; done sequencing_summary.txt gzip sequencing_summary.txt You should then have something like this: \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 fast5/ # fast5 folder | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... Inputs fast5_fetcher.py takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz) 1. fastq, paf, or flat This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2 . This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not. 2. Sequencing summary The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not. 3. Building the index How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures. - Raw structure (not preferred) for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index - Local basecalled structure for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index - Parallel basecalled structure for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index","title":"Files and folders"},{"location":"files/#overview","text":"Most files and formats are straightforward. The only tricky starting point is fast5_fetcher.py . This becomes even more important if using something like batch_tater.py with SGE to significantly reduce the time taken to run. Once processing more than 50k files, the batch_tater.py method becomes very attractive.","title":"Overview"},{"location":"files/#getting-started","text":"Building an index of fast5 files and their paths, as well as a simple bash script to control the workflow, be it on a local machine, or HPC, will depend on the starting file structure.","title":"Getting Started"},{"location":"files/#file-structures","text":"I have endeavoured to include a few diverse uses, starting from different file states, but of course, I can't think of everything, so if there is something you wish to accomplished with fast5_fetcher.py , but can't quite get it to work for you, let me know, and we can try to work it out.","title":"File structures"},{"location":"files/#1-raw-structure","text":"This is the most basic structure, where all files are present in an accessible state. \u251c\u2500\u2500 huntsman.fastq \u251c\u2500\u2500 sequencing_summary.txt \u251c\u2500\u2500 huntsman_reads/ # Read folder \u2502 \u251c\u2500\u2500 0/ # individual folders containing ~4000 fast5s | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ...","title":"1. Raw structure"},{"location":"files/#2-local-basecalled-structure","text":"This structure is the typical structure post local basecalling fastq and sequencing_summary files have been gzipped and the folders in the reads folder have been tarballed into one large file \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 huntsman_reads.tar # Tarballed read folder | # Tarball expanded |-- \u2502 \u251c\u2500\u2500 0/ # individual folders inside tarball | | \u251c\u2500\u2500 huntsman_read1.fast5 | | \u2514\u2500\u2500 huntsman_read2.fast5 | | \u2514\u2500\u2500 ... | \u251c\u2500\u2500 1/ | | \u251c\u2500\u2500 huntsman_read#.fast5 | | \u2514\u2500\u2500 ... \u2514\u2500\u2500 \u251c\u2500\u2500 ...","title":"2. Local basecalled structure"},{"location":"files/#3-parallel-basecalled-structure","text":"This structure is post massively parallel basecalling, and looks like multiples of the above structure. \u251c\u2500\u2500 fastq/ | \u251c\u2500\u2500 huntsman.1.fastq.gz | \u2514\u2500\u2500 huntsman.2.fastq.gz | \u2514\u2500\u2500 huntsman.3.fastq.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 logs/ | \u251c\u2500\u2500 sequencing_summary.1.txt.gz | \u2514\u2500\u2500 sequencing_summary.2.txt.gz | \u2514\u2500\u2500 sequencing_summary.3.txt.gz | \u2514\u2500\u2500 ... \u251c\u2500\u2500 fast5/ | \u251c\u2500\u2500 1.tar | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ... With this structure, combining the .fastq and sequencing_summary.txt.gz files is needed.","title":"3. Parallel basecalled structure"},{"location":"files/#combine-fastqgz-files","text":"for file in fastq/*.fastq.gz; do cat $file; done huntsman.fastq.gz","title":"Combine fastq.gz files"},{"location":"files/#combine-sequencing_summarytxtgz-files","text":"# create header zcat $(ls logs/sequencing_summary*.txt.gz | head -1) | head -1 sequencing_summary.txt # combine all files, skipping first line header for file in logs/sequencing_summary*.txt.gz; do zcat $file | tail -n +2; done sequencing_summary.txt gzip sequencing_summary.txt You should then have something like this: \u251c\u2500\u2500 huntsman.fastq.gz # gzipped \u251c\u2500\u2500 sequencing_summary.txt.gz # gzipped \u251c\u2500\u2500 fast5/ # fast5 folder | \u251c\u2500\u2500 1.tar # each tar contains ~4000 fast5 files | \u2514\u2500\u2500 2.tar | \u2514\u2500\u2500 3.tar | \u2514\u2500\u2500 ...","title":"Combine sequencing_summary.txt.gz files"},{"location":"files/#inputs","text":"fast5_fetcher.py takes 3 files as input: fastq, paf, or flat (.gz) sequencing_summary.txt(.gz) name.index(.gz)","title":"Inputs"},{"location":"files/#1-fastq-paf-or-flat","text":"This is where the readIDs are collected, to be matched with their respective fast5 files for fetching. The idea being, that some form of selection has occurred to generate the files. In the case of a fastq , it may be filtered for all the reads above a certain quality, or from a particular barcode after running barcode detection. For the paf file, it is an alignment output of minimap2 . This can be used to fetch only the fast5 files that align to some reference, or has been filtered to only contain the reads that align to a particular region of interest. A flat file in this case is a file that contains a list of readIDs, one on each line. This allows the user to generate any list of reads to fetch from any other desired method. Each of these files can be gzipped or not.","title":"1. fastq, paf, or flat"},{"location":"files/#2-sequencing-summary","text":"The sequencing_summary.txt file is created by the basecalling software, (Albacore, Guppy), and contains information about each read, including the readID and fast5 file name, along with length, quality scores, and potentially barcode information. There is a shortcut method in which you can use the sequencing_summary.txt only, without the need for a fastq, paf, or flat file. In this case, leave the -q , -f , -r fields empty. This file can be gzipped or not.","title":"2. Sequencing summary"},{"location":"files/#3-building-the-index","text":"How the index is built depends on which file structure you are using. It will work with both tarred and un-tarred file structures.","title":"3. Building the index"},{"location":"files/#-raw-structure-not-preferred","text":"for file in $(pwd)/reads/*/*;do echo $file; done name.index gzip name.index","title":"- Raw structure (not preferred)"},{"location":"files/#-local-basecalled-structure","text":"for file in $(pwd)/reads.tar; do echo $file; tar -tf $file; done name.index gzip name.index","title":"- Local basecalled structure"},{"location":"files/#-parallel-basecalled-structure","text":"for file in $(pwd)/fast5/*fast5.tar; do echo $file; tar -tf $file; done name.index If you have multiple experiments, then cat them all together and gzip. for file in ./*.index; do cat $file; done ../all.name.index gzip all.name.index","title":"- Parallel basecalled structure"},{"location":"install/","text":"Installation Requirements Following a self imposed guideline, most things written to handle nanopore data or bioinformatics in general, will use as little 3rd party libraries as possible, aiming for only core libraries, or have all included files in the package. In the case of fast5_fetcher.py and batch_tater.py , only core python libraries are used. So as long as Python 2.7+ is present, everything should work with no extra steps. There is one catch. Everything is written primarily for use with Linux . Due to MacOS running on Unix, so long as the GNU tools are installed (see below), there should be minimal issues running it. Windows however may require more massaging. SquiggleKit tools were not made to be executable to allow for use with varying python environments on various operating systems. To make them executable, add #! paths, such as #!/usr/bin/env python2.7 as the first line of each of the files, then add the SquiggleKit directory to the PATH variable in ~/.bashrc , export PATH=\"$HOME/path/to/SquiggleKit:$PATH\" Install git clone https://github.com/Psy-Fer/SquiggleKit.git pip install numpy h5py sklearn matplotlib Quick start fast5_fetcher If using MacOS, and NOT using homebrew, install it here: homebrew installation instructions then install gnu-tar with: brew install gnu-tar Basic use on a local computer fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 SquigglePull All raw data: python SquigglePull.py -rv -p ~/data/test/reads/1/ -f all data.tsv Positional event data: python SquigglePull.py -ev -p ./test/ -t 50,150 -f pos1 data.tsv SquigglePlot Individual File full signal python SquigglePlot.py -i ~/data/test.fast5 Plot all from top folder in green python SquigglePlot.py -p ~/data/ --plot_colour -g Plot first 2000 data points of each read from signal file and save at 300dpi pdf * python SquigglePlot.py -s signals.tsv.gz --plot_colour teal -n 2000 --dpi 300 --no_show o--save test.pdf --save_path ./test/plots/ segmenter Stall identification python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv MotifSeq Nanopore adapter identification python MotifSeq.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model -t 120 -d 120 signals_adapters.tsv Full requirements fast5_fetcher,py : core python libraries SquigglePull.py : numpy h5py sklearn pip install numpy h5py sklearn SquigglePlot.py : numpy matplotlib h5py pip install numpy h5py matplotlib segmenter.py : numpy matplotlib h5py sklearn pip install numpy h5py sklearn matplotlib MotifSeq.py : numpy h5py sklearn matplotlib mlpy 3.5.0 (don't use pip for this) pip install numpy h5py sklearn matplotlib Installing mlpy: Download the Files Install Instructions","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#requirements","text":"Following a self imposed guideline, most things written to handle nanopore data or bioinformatics in general, will use as little 3rd party libraries as possible, aiming for only core libraries, or have all included files in the package. In the case of fast5_fetcher.py and batch_tater.py , only core python libraries are used. So as long as Python 2.7+ is present, everything should work with no extra steps. There is one catch. Everything is written primarily for use with Linux . Due to MacOS running on Unix, so long as the GNU tools are installed (see below), there should be minimal issues running it. Windows however may require more massaging. SquiggleKit tools were not made to be executable to allow for use with varying python environments on various operating systems. To make them executable, add #! paths, such as #!/usr/bin/env python2.7 as the first line of each of the files, then add the SquiggleKit directory to the PATH variable in ~/.bashrc , export PATH=\"$HOME/path/to/SquiggleKit:$PATH\"","title":"Requirements"},{"location":"install/#install","text":"git clone https://github.com/Psy-Fer/SquiggleKit.git pip install numpy h5py sklearn matplotlib","title":"Install"},{"location":"install/#quick-start","text":"","title":"Quick start"},{"location":"install/#fast5_fetcher","text":"If using MacOS, and NOT using homebrew, install it here: homebrew installation instructions then install gnu-tar with: brew install gnu-tar","title":"fast5_fetcher"},{"location":"install/#basic-use-on-a-local-computer","text":"fastq python fast5_fetcher.py -q my.fastq.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 paf python fast5_fetcher.py -p my.paf -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 flat python fast5_fetcher.py -f my_flat.txt.gz -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5 sequencing_summary.txt only python fast5_fetcher.py -s sequencing_summary.txt.gz -i name.index.gz -o ./fast5","title":"Basic use on a local computer"},{"location":"install/#squigglepull","text":"All raw data: python SquigglePull.py -rv -p ~/data/test/reads/1/ -f all data.tsv Positional event data: python SquigglePull.py -ev -p ./test/ -t 50,150 -f pos1 data.tsv","title":"SquigglePull"},{"location":"install/#squiggleplot","text":"Individual File full signal python SquigglePlot.py -i ~/data/test.fast5 Plot all from top folder in green python SquigglePlot.py -p ~/data/ --plot_colour -g Plot first 2000 data points of each read from signal file and save at 300dpi pdf * python SquigglePlot.py -s signals.tsv.gz --plot_colour teal -n 2000 --dpi 300 --no_show o--save test.pdf --save_path ./test/plots/","title":"SquigglePlot"},{"location":"install/#segmenter","text":"Stall identification python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv","title":"segmenter"},{"location":"install/#motifseq","text":"Nanopore adapter identification python MotifSeq.py -s signals.tsv.gz --segs signals_stall_segments.tsv -a adapter.model -t 120 -d 120 signals_adapters.tsv","title":"MotifSeq"},{"location":"install/#full-requirements","text":"fast5_fetcher,py : core python libraries SquigglePull.py : numpy h5py sklearn pip install numpy h5py sklearn SquigglePlot.py : numpy matplotlib h5py pip install numpy h5py matplotlib segmenter.py : numpy matplotlib h5py sklearn pip install numpy h5py sklearn matplotlib MotifSeq.py : numpy h5py sklearn matplotlib mlpy 3.5.0 (don't use pip for this) pip install numpy h5py sklearn matplotlib","title":"Full requirements"},{"location":"install/#installing-mlpy","text":"Download the Files Install Instructions","title":"Installing mlpy:"},{"location":"segmenter/","text":"Segmenter Background Nanopore signal can have larger structurally identifiable regions. These regions can be separated into 3 categories: Stall, homopolymer, and regular DNA/cDNA sequence. Identifying the stall at the start of a nanopore read allows for extraction, visualisation, and comparison of the regular sequence. Finding homopolymer sequences can aid in identifying full lenght cDNA reads, breaking up contatonated reads, or identifying adjacent regions. Using segmenter in conjunction with MotifSeq allows for simple targeting within the raw signal. Selected regions can then be, for example, used as input to various classical comparison methods, such as various types dynamic programming, or used as training sets for Machine/Deep Learning techniques. The core algorithm calculates the median of the full read, sets thresholds about the median using a fraction of the standard deviation, and uses a sliding window to find regions of signal which stay inside the threshold region. Error correction and segment merging helps handle the noisy nature of the raw signal data. This is a demonstration for analysing raw signal data. The algorithm was developed out frustration with the overcomplicated or incompatible methods employed in other time analysis fields, such as sound wave or ECG detection. It is has undergone minimal optimisation, though still gives surprisingly good results. Getting Started Segmenter takes a flat file list of fast5 paths, a top directory of file paths, or a signal file from SquigglePull . It can be set to only detect stalls, or it can detect homopolymers within a distance of the start of a read depending on read structure. Many of the core arguments in the algorithm can be modified with arguments for tuning output. Instructions for use Segmenter has a built visualisation using -v . This is very helpful in tuning the parameters. Default values are a rough estimate for getting some results from most reads. Quick start Identify any segments in folder and visualise each one Use f to full screen a plot, and ctrl+w to close a plot and move to the next one. python segmenter.py -p ./test/ -v Stall identification python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv Full usage usage: segmenter.py [-h] [-f F5F | -p F5_PATH | -s SIGNAL] [-n NUM] [-e ERROR] [-c CORRECTOR] [-w WINDOW] [-d SEG_DIST] [-t STD_SCALE] [-v] [-g] [-b GAP_DIST] [-k] [-u] [-l STALL_LEN] [-j STALL_START] [-scale_hi SCALE_HI] [-scale_low SCALE_LOW] segmenter - script to find obvious regions in squiggle data optional arguments: -h, --help show this help message and exit -f F5F, --f5f F5F File list of fast5 paths -p F5_PATH, --f5_path F5_PATH Fast5 top dir -s SIGNAL, --signal SIGNAL Extracted signal file from squigglePull -n NUM, --Num NUM Section of signal to look at - default 0=all -e ERROR, --error ERROR Allowable error in segment algorithm -c CORRECTOR, --corrector CORRECTOR Window size for increasing total error correction - better long segment detection -w WINDOW, --window WINDOW Minimum segment window size to be detected -d SEG_DIST, --seg_dist SEG_DIST Maximum distance between 2 segments to be merged into 1 -t STD_SCALE, --std_scale STD_SCALE Scale factor of STDev about median -v, --view view each output -g, --gap Turn on gap distance for stall to polyTAil -b GAP_DIST, --gap_dist GAP_DIST Maximum distance between stall and polyTAil segment - for 10X/dRNA -k, --stall Turn on stall detection - must be present -u, --test Run Tests -l STALL_LEN, --stall_len STALL_LEN Minimum percentage of minimum window segment for initial stall segment -j STALL_START, --stall_start STALL_START Maximum distance for start of stall segment to be detected -scale_hi SCALE_HI, --scale_hi SCALE_HI Upper limit for signal outlier scaling -scale_low SCALE_LOW, --scale_low SCALE_LOW Lower limit for signal outlier scaling Detailed Parameter descriptions Segmenter Parameters Default Description -n --Num 0 This allows for the signal to be trimmed a static value. Default is 0 which means get all signal. This could be to limit the search space for segments, or to bring the start of a read into focus using -v, --view . For example, -n 2000 would cut the raw signal after 2000 datapoints. This may impact where segments are found, so it's best to use this only for testing purpose. -w --window 200 The minimum size of a segment that will be detected. Larger structural sections like stalls and homopolymers are significantly bigger than anything else in the signal. However this can vary for a number of reasons, such as the average length of a polyA tail. Stalls can be almost any size. See -l, --stall_len to see how this is handled. Decreasing this value will increase sensitivity, but decrease accuracy. Increasing it will increase accuracy, but reduce sensitivity. -t --std_scale 0.75 To detect a segment, a simple yet effective filter is run across the signal. First it calculates the median of the read, then sets two thresholds above and below the median by taking the standard deviation of the read and scaling it by this scale factor. A segment is defined as a region that fluctuates between these two thresholds for at least the minimum window size, -w --window . -l --stall_len 0.25 The stall could be a large range of lengths, including quite a fraction smaller than the minimum window for a homopolymer region. Currently, this value just scales the window length, however this may not be the best approach as the window gets very large or very small. -e --error 5 Error is defined as the total number of times the signal can rise above or below the thresholds set above and not break the segment detection. This can be modified by other settings. -c --corrector 50 The corrector is for helping with long segment detection. Once the segment currently being detected passes the -w --window length, this kicks in and starts increasing the total allowable error for every X values it extends. -d --seg_dist 50 If the start of a new segment is within this distance of the last detected segment, it will merge the two segments toghers. This especially helps with long segments that have a significant disruption in them that the error correction can't account for without impacting accuracy. -scale_low -scale_hi 0, 900 Throughout nanopore raw signal are large spikes which can impact calculated values such as median and standard deviation. This scale factor helps limit the region in which signal will be included. Note : it removes the signals that go out of bounds. The defaults are good for DNA and cDNA. For RNA, -scale_hi 1200 is more appropriate. To stop this from working, set both values to -50000 and 50000 respectively. User based tests users can modify the test_segs function to automate their own tests -u --test False Turn on user tests. This will allow for various tests to be done on the segments. A few have been added for some ideas and how to integrate them into the script. -k --stall False Use to turn on Stall detection. Ie, the start position of the first detected segment must be within a given distance. -j --stall_start 300 The maximum distance from the start of a read the start position of the first detected segment must be within a given distance. -g --gap False Turns on gap distance for detecting a polyTAil within a certain distance of a detected stall -b -gap_dist 3000 The maximum distance the end of a stall segment can be from the start of the next segment. useful for finding polyT/A tails within an expected search space.","title":"segmenter"},{"location":"segmenter/#segmenter","text":"","title":"Segmenter"},{"location":"segmenter/#background","text":"Nanopore signal can have larger structurally identifiable regions. These regions can be separated into 3 categories: Stall, homopolymer, and regular DNA/cDNA sequence. Identifying the stall at the start of a nanopore read allows for extraction, visualisation, and comparison of the regular sequence. Finding homopolymer sequences can aid in identifying full lenght cDNA reads, breaking up contatonated reads, or identifying adjacent regions. Using segmenter in conjunction with MotifSeq allows for simple targeting within the raw signal. Selected regions can then be, for example, used as input to various classical comparison methods, such as various types dynamic programming, or used as training sets for Machine/Deep Learning techniques. The core algorithm calculates the median of the full read, sets thresholds about the median using a fraction of the standard deviation, and uses a sliding window to find regions of signal which stay inside the threshold region. Error correction and segment merging helps handle the noisy nature of the raw signal data. This is a demonstration for analysing raw signal data. The algorithm was developed out frustration with the overcomplicated or incompatible methods employed in other time analysis fields, such as sound wave or ECG detection. It is has undergone minimal optimisation, though still gives surprisingly good results.","title":"Background"},{"location":"segmenter/#getting-started","text":"Segmenter takes a flat file list of fast5 paths, a top directory of file paths, or a signal file from SquigglePull . It can be set to only detect stalls, or it can detect homopolymers within a distance of the start of a read depending on read structure. Many of the core arguments in the algorithm can be modified with arguments for tuning output.","title":"Getting Started"},{"location":"segmenter/#instructions-for-use","text":"Segmenter has a built visualisation using -v . This is very helpful in tuning the parameters. Default values are a rough estimate for getting some results from most reads.","title":"Instructions for use"},{"location":"segmenter/#quick-start","text":"Identify any segments in folder and visualise each one Use f to full screen a plot, and ctrl+w to close a plot and move to the next one. python segmenter.py -p ./test/ -v Stall identification python segmenter.py -s signals.tsv.gz -ku -j 100 signals_stall_segments.tsv","title":"Quick start"},{"location":"segmenter/#full-usage","text":"usage: segmenter.py [-h] [-f F5F | -p F5_PATH | -s SIGNAL] [-n NUM] [-e ERROR] [-c CORRECTOR] [-w WINDOW] [-d SEG_DIST] [-t STD_SCALE] [-v] [-g] [-b GAP_DIST] [-k] [-u] [-l STALL_LEN] [-j STALL_START] [-scale_hi SCALE_HI] [-scale_low SCALE_LOW] segmenter - script to find obvious regions in squiggle data optional arguments: -h, --help show this help message and exit -f F5F, --f5f F5F File list of fast5 paths -p F5_PATH, --f5_path F5_PATH Fast5 top dir -s SIGNAL, --signal SIGNAL Extracted signal file from squigglePull -n NUM, --Num NUM Section of signal to look at - default 0=all -e ERROR, --error ERROR Allowable error in segment algorithm -c CORRECTOR, --corrector CORRECTOR Window size for increasing total error correction - better long segment detection -w WINDOW, --window WINDOW Minimum segment window size to be detected -d SEG_DIST, --seg_dist SEG_DIST Maximum distance between 2 segments to be merged into 1 -t STD_SCALE, --std_scale STD_SCALE Scale factor of STDev about median -v, --view view each output -g, --gap Turn on gap distance for stall to polyTAil -b GAP_DIST, --gap_dist GAP_DIST Maximum distance between stall and polyTAil segment - for 10X/dRNA -k, --stall Turn on stall detection - must be present -u, --test Run Tests -l STALL_LEN, --stall_len STALL_LEN Minimum percentage of minimum window segment for initial stall segment -j STALL_START, --stall_start STALL_START Maximum distance for start of stall segment to be detected -scale_hi SCALE_HI, --scale_hi SCALE_HI Upper limit for signal outlier scaling -scale_low SCALE_LOW, --scale_low SCALE_LOW Lower limit for signal outlier scaling","title":"Full usage"},{"location":"segmenter/#detailed-parameter-descriptions","text":"Segmenter Parameters Default Description -n --Num 0 This allows for the signal to be trimmed a static value. Default is 0 which means get all signal. This could be to limit the search space for segments, or to bring the start of a read into focus using -v, --view . For example, -n 2000 would cut the raw signal after 2000 datapoints. This may impact where segments are found, so it's best to use this only for testing purpose. -w --window 200 The minimum size of a segment that will be detected. Larger structural sections like stalls and homopolymers are significantly bigger than anything else in the signal. However this can vary for a number of reasons, such as the average length of a polyA tail. Stalls can be almost any size. See -l, --stall_len to see how this is handled. Decreasing this value will increase sensitivity, but decrease accuracy. Increasing it will increase accuracy, but reduce sensitivity. -t --std_scale 0.75 To detect a segment, a simple yet effective filter is run across the signal. First it calculates the median of the read, then sets two thresholds above and below the median by taking the standard deviation of the read and scaling it by this scale factor. A segment is defined as a region that fluctuates between these two thresholds for at least the minimum window size, -w --window . -l --stall_len 0.25 The stall could be a large range of lengths, including quite a fraction smaller than the minimum window for a homopolymer region. Currently, this value just scales the window length, however this may not be the best approach as the window gets very large or very small. -e --error 5 Error is defined as the total number of times the signal can rise above or below the thresholds set above and not break the segment detection. This can be modified by other settings. -c --corrector 50 The corrector is for helping with long segment detection. Once the segment currently being detected passes the -w --window length, this kicks in and starts increasing the total allowable error for every X values it extends. -d --seg_dist 50 If the start of a new segment is within this distance of the last detected segment, it will merge the two segments toghers. This especially helps with long segments that have a significant disruption in them that the error correction can't account for without impacting accuracy. -scale_low -scale_hi 0, 900 Throughout nanopore raw signal are large spikes which can impact calculated values such as median and standard deviation. This scale factor helps limit the region in which signal will be included. Note : it removes the signals that go out of bounds. The defaults are good for DNA and cDNA. For RNA, -scale_hi 1200 is more appropriate. To stop this from working, set both values to -50000 and 50000 respectively. User based tests users can modify the test_segs function to automate their own tests -u --test False Turn on user tests. This will allow for various tests to be done on the segments. A few have been added for some ideas and how to integrate them into the script. -k --stall False Use to turn on Stall detection. Ie, the start position of the first detected segment must be within a given distance. -j --stall_start 300 The maximum distance from the start of a read the start position of the first detected segment must be within a given distance. -g --gap False Turns on gap distance for detecting a polyTAil within a certain distance of a detected stall -b -gap_dist 3000 The maximum distance the end of a stall segment can be from the start of the next segment. useful for finding polyT/A tails within an expected search space.","title":"Detailed Parameter descriptions"}]}